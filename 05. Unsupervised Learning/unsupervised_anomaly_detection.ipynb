{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Neural Networks for Anomaly Detection\n",
    "\n",
    "What if density estimation is not good enough to identify the outliers in our data? Could we used an actual neural network for this purpose?\n",
    "\n",
    "Recall from previous tutorials how Neural Networks rely on a loss function for the optimization steps involved in the training process.\n",
    "Could we have a loss function without a target for our output? No, not really, but we don't need actual labels to be able to give our neural network a target output. \n",
    "\n",
    "What if our target is the same as our input? ðŸ¤¯\n",
    "\n",
    "So far this just sounds like a glorified and computationally expensive identity function, but we have one more trick up our sleeve. Enter **the autoencoder** . . . \n",
    "\n",
    "## The Autoencoder \n",
    "\n",
    "This neural network may be thought of as having two independent parts:\n",
    "- **the Encoder**: that takes our initial input and creates a representation of it in a space with a smaller number of dimensions\n",
    "- **the Decoder**: that takes the *latent representation*, or the encoder output, and tries to reconstruct the original input \n",
    "\n",
    "By adding this *latent space*, we force our network to effectively compress our data. This *informational bottleneck* is what prevents the network from just becoming an identity function.\n",
    "\n",
    "<img src=\"./resources/ae.svg\" width=400 style=\"background-color:white;\">\n",
    "\n",
    "Thus, we don't need labels anymore, we train the network to create a smaller-dimensional representation of our inputs and reconstruct them back from there. We could use any distance metric as our loss function, such as the *Mean Squared Error*.\n",
    "\n",
    "Now you may ask what does this compression method have to do with detecting anomalies in our data?\n",
    "\n",
    "Well, the main assumption of this model is that our data can be compressed, which implies that it has an internal structure that may be represented with fewer, more expressive variables. If we were to run samples that are significantly different from what the model learned, it will not be as good as reconstructing them. So basically we may use the reconstruction error between the input and the output as sort of an **anomaly score**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on: Autoencoder for credit card fraud detection\n",
    "\n",
    "### The Data\n",
    "The objective of this exercise is to build a model able to detect fraudulent credit card transactions among normal transactions. For this we train a special type of neural network called autoencoder. This network has as many input nodes as output nodes, and several hidden layers with, usually lower dimensions. \n",
    "\n",
    "The dataset we're going to use can be downloaded from [Kaggle](https://www.kaggle.com/mlg-ulb/creditcardfraud) (big file: 144 MB). It contains data about credit card transactions that occurred during a period of two days, with 492 frauds out of 284,807 transactions.\n",
    "\n",
    "All 30 features in the dataset are numerical. The data has been transformed using PCA transformation(s) due to privacy reasons. The two features that haven't been changed are Time and Amount. Time contains the seconds elapsed between each transaction and the first transaction in the dataset.\n",
    "\n",
    "The dataset also contains the class of event: 0 = normal transaction; 1 = fraudulent transaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The tasks\n",
    "For this complete exercise you have nothing already set-up for you. Use what you already learned to complete all the tasks. Some of the tasks include things that were not discussed in the previous tutorials, but have references to resources where you may learn about them. Do as much as you can and remember: *Google is your friend*\n",
    "## 0. Import Libraries you may need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import anything you may need\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Explore Data\n",
    "\n",
    "a) Download the dataset and load it in a pandas dataframe. Look at the first 10 examples.\n",
    "\n",
    "b) Separate the data in two classes `normal` and `fraud`, then remove the class label from these datasets in order to conserve only the features.\n",
    "\n",
    "c) Plot the first 5 features of both normal and fraud data (plotting all features is time consuming).\n",
    "\n",
    "d) Split the `normal` dataset into a training and a test sample (each of same size).\n",
    "\n",
    "After the last step you should have 3 datasets:\n",
    "* normal data used for training\n",
    "* normal data used for testing\n",
    "* fraud data used for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the given dataset\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Rescale data\n",
    "\n",
    "Since features have different range we apply a transformation to each feature. For this we  use the MinMaxScaler that scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one:\n",
    "\n",
    "See: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
    "\n",
    "a) Fit and transofrm the training dataset using the scaler with the `fit_transform` method.\n",
    "\n",
    "b) Apply the transformation on the tests samples using the `transform` method.\n",
    "\n",
    "c) Plot the first 5 features of the normal and fraud test data and see how they changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale the data\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Partition training data\n",
    "\n",
    "After all of this, it's important to partition the data. In order for your model to generalize well, you split the training data into two parts: a training and a validation set. You will train your model on 80% of the data and validate it on 20% of the remaining training data. The validation dataset is just like a test set that gets evaluated during training. You will give this validation sample to the `fit` method of your model. You may take a look [here](https://www.tensorflow.org/guide/keras/train_and_evaluate#using_a_validation_dataset) to get a better idea on how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition the training dataset\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Â AutoEncoder model\n",
    "\n",
    "Now we create the AutoEncoder model. \n",
    "\n",
    "Complete the network structure below using `Dense` layers and sigmoid activation functions:\n",
    "\n",
    "a) in the encoding part create 3 layers of dimension 30, 25 and 20, each with a sigmoid activation function\n",
    "\n",
    "b) in the decoding part create 2 layers of dimension 25, 30, where only the 1st layer has a sigmoid activation function\n",
    "\n",
    "The encoder and decoder may be part of the same model, for an easier model definition. You may just stack all the layers together in `Sequential` model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model here\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training on normal samples\n",
    "\n",
    "Compile the model and run the training of the network on the training sample. For this complete the code below by answering the following questions:\n",
    "\n",
    "a) Choose the mean square error loss function.\n",
    "\n",
    "b) Select the Adam optimizer method with a learning rate of 0.001 and compile the model.\n",
    "\n",
    "c) Fit the model using the training and validation data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model here\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Calculate autoencoder distances\n",
    "\n",
    "Now we calculate the euclidean distance between the autoencoder input and output.\n",
    "\n",
    "$$ \\text{distance} = \\sqrt{ ||x_{\\text{input}} - x_{\\text{output}}||^2} = \\sqrt{ \\sum_i (x^i_{\\text{input}} - x^i_{\\text{output}})^2}$$\n",
    "\n",
    "a) Compute those distances on both the normal test data and the fraud test data.\n",
    "\n",
    "b) Plot the histograms of the calculated distances of the normal and fraud test data. For better viewing choose a logarithmic scale for the y axis. Comment on the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train the model here\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Confusion matrix\n",
    "\n",
    "A confusion matrix is a good way of visualizing how good our model is at classification. Check out the [wikipedia](https://en.wikipedia.org/wiki/Confusion_matrix) page to get a better idea about it.\n",
    "\n",
    "Take a look at [`sklearn.metrics.confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) to find out how to compute one easily. \n",
    "\n",
    "Build a confusion matrix with a threshold on the distance such that 50% of fraud transactions are detected. What is the true positive rate in this case ? Is this threshold interesting ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a confusion matrix\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ROC Curve\n",
    "\n",
    "Another useful way of visualizing performance is through the use of ROC-curves. You can check what those are [here](https://en.wikipedia.org/wiki/Receiver_operating_characteristic). \n",
    "\n",
    "Note that `scikit-learn` has functions to calculate those as well. You just need to figure out how to use them. Maybe [this page](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html) can help you.\n",
    "\n",
    "Draw the ROC curve for the test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a confusion matrix\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Optimize the performance of the model (optional)\n",
    "\n",
    "Try the following:\n",
    "* Change hyperparameters values\n",
    "* Modify activation functions\n",
    "* Add one or more layers\n",
    "* Try [dropout](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout)\n",
    "* Anything else you may think of . . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional optimization\n",
    "raise NotImplementedError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
