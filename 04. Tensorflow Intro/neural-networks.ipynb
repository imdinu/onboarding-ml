{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Neural networks in Tensorflow\n",
    "\n",
    "Seminar outline\n",
    "- Why Neural Networks?\n",
    "- Keras API\n",
    "- TensorBoard\n",
    "\n",
    "You may need to pip install in your environment the following packages: `tensorflow-addons`, `tqdm` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From linear models to neural networks\n",
    "\n",
    "As we observed in the previous examples, feature selection and engineering can make or break your linear model. Although it seems easy on toy problems, such as the example below, finding good features becomes a highly non-trivial task when working with more complex data.\n",
    "<img src=\"resources/features.png\" width=600>\n",
    "\n",
    "**Could we also automate feature engineering?**\n",
    "\n",
    "Let's say we'll add another linear model just to perform feature selection and use the output of the first model as the input for predictions.\n",
    "<img src=\"resources/linear_stack.png\" width=600>\n",
    "\n",
    "In this case, what would our prediction $\\hat{y}$ look like?\n",
    "$$ \\hat{y} = w^T\\tilde{x} = w^T(Wx) = (w^TW)x = w'^Tx$$\n",
    "\n",
    "The matrix $W$ and vector $w$ just get multiplied, resulting in the linear coefficients vector $w'$. Most optimization algorithms would yeld the same values for $w$ and $w'$. Stacking two linear models together is just like having a single one.  Note that for our toy example above, the \"good feature\" was **not linear** with respect to the original ones. \n",
    "\n",
    "Then, let's try to add a non-linearity:\n",
    "\n",
    "<img src=\"resources/nonlinear_stack.png\" width=600>\n",
    "\n",
    "where $g(x)$ is some **nonlinear scalar function** applied **element-wise**.\n",
    "\n",
    "Now our prediction looks a bit different, the two sets of weights can not be merged into a single one anymore:\n",
    "$$ \\hat{y} = w^T\\tilde{x} = w^Tg(Wx) $$\n",
    "\n",
    "Congratulations! We just created a basic **neural network**. A more detailed representation of what we did would look like this:\n",
    "<img src=\"resources/neural_network.png\" width=600>\n",
    "\n",
    "As the specific terminology goes, we have:\n",
    "- **Neurons:** The values represented by $x_i$ or $\\tilde{x}_i$. Also the output $\\hat{y}$ is considered a neuron.\n",
    "- **Layers:** The column of $x_i$ values is considered the *input layer*, $\\tilde{x}_i$ are part of a *hidden layer* and the single $\\hat{y}$ value is considered the *output layer*\n",
    "- **Weights:** Usually denoted by $W_{ij}$ or $w_i$\n",
    "- **Activation Function:** Our nonlinear scalar function $g$. Confusingly enough \"the neuron's activation\" (not activation function) is the value that gets computed inside that neuron (a.k.a the result of the application of the activation function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "\n",
    "Any continuous and differentiable non-linear function should be fine for our activation, but if we take into account some pragmatic aspects such as the complexity of computation and the behavior of the derivative, there are a handful that used most of the time:\n",
    "- $\\text{sigmoid}(x)=\\frac{1}{1+e^{-x}}$\n",
    "- $\\text{tanh}(x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}$\n",
    "- $\\text{ReLU}(x)=\\mathrm{max}(0, x)$\n",
    "  \n",
    "Of course, there are others, but for now let's try and see what those three look like. They are already implemented can use the Tensorflow neural network submodule to access `tf.nn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 1000)\n",
    "sigmoid = tf.nn.sigmoid(x)\n",
    "tanh = tf.nn.tanh(x)\n",
    "relu = tf.nn.relu(x)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.grid()\n",
    "plt.plot(x, sigmoid.numpy(), label=\"sigmoid\")\n",
    "plt.plot(x, tanh.numpy(), label=\"tanh\")\n",
    "plt.plot(x, relu.numpy(), label=\"relu\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So what is all this *deep learning* that I keep hearing about?\n",
    "\n",
    "Mathematically speaking, this neural network with a single *hidden layer* is an **universal function approximator**. Since we could make the size of our hidden layer arbitrarily large, we could, in theory approximate any function with arbitrarily good precision. In weight matrix between two layers always has a dimension equal to $d \\times d'$. So if we want, we could make the hidden layer have 1000 nodes (or neurons), but if our input has 10 nodes, we will arrive at 10000 parameters to optimize.\n",
    "\n",
    "However, similar levels of performance could be expected if we just have 2 layers of 10 neurons each. This results in $10 \\times 10 \\times 2 = 200$ parameters to optimize. This is why, in practice is better to go deeper rather than wider. \n",
    "<img src=\"resources/deep_net.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-on example\n",
    "\n",
    "Let's generate some toy data with a 2D function that has a binary output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@np.vectorize\n",
    "def some_2d_function(x, y, noise=0):\n",
    "    eps = np.random.normal(0,noise)\n",
    "    y = y+0.4\n",
    "    z = np.sin(7*x**2-eps)+np.cos(7*y**2+eps)+(x**2+y**2)-np.exp(x*y)\n",
    "    return x**2 if x**2>(y) else z\n",
    "\n",
    "@np.vectorize\n",
    "def some_2d_function_binary(x, y, noise=0):\n",
    "    z = some_2d_function(x, y, noise)\n",
    "    return 0 if z > 0 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_low, u_high, u_step = -1, 1, 0.02\n",
    "v_low, v_high, v_step = 0, 1, 0.01\n",
    "u, v = np.meshgrid(np.arange(u_low, u_high, u_step), \n",
    "                     np.arange(v_low, v_high, v_step))\n",
    "y = some_2d_function(u,v)\n",
    "y_binary = some_2d_function_binary(u,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We end up with two categories of points distributed as seen below. The true categories as defined by the analytical function are separated by the contour, but our samples are a bit noisy (as real data usually is) so they may end up falling outside of the contour sometimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_sample = np.random.uniform(u_low, u_high, 5_000)\n",
    "v_sample = np.random.uniform(v_low, v_high, 5_000)\n",
    "y_sample = some_2d_function_binary(u_sample, v_sample, noise=0.2)\n",
    "fig = plt.figure()\n",
    "plt.contour(u, v, y_binary)\n",
    "plt.scatter(u_sample, v_sample, c=y_sample, s=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need to separate our training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack([u_sample, v_sample]).T\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_sample, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow keras API\n",
    "\n",
    "We are going to use `keras` which is a **high-level API** for defining and training neural networks Tensorflow. Setting up the parameters, applying activation functions, differentiation and computing the gradients to update parameters are all handled under the hood by `keras` using **low-level** Tensorflow. In `kears` we work with higher level objects such as `Layers` and `Models`.\n",
    "\n",
    "First let's define our model as a `tf.keras.Sequential()` object. Then we use the `add` method to set up the `Layers` which will be part of the model. There are several types of layers defined in `keras`, but for now we just need `Dense` and `Activation`, as we showed in the introductory explanation.\n",
    "\n",
    "**Notes:**\n",
    "- only the first layer needs an `input_shape` specification, the rest of them an considered to have the previous layer as input, by default\n",
    "- the hidden layer has `relu` as the activation function, but for the output `sigmoid` is more convenient, since it returns values in the $(0, 1)$ interval, and our true labels are either 0 or 1\n",
    "- the dense layer also has a constant term called *bias* $b$ for every neuron. Thus to compute the value of the $j$-th neuron on the layer $k$, before applying the activation function, we have:\n",
    "  $$  x_j^{(k)} = (\\sum_i^d W_{ij}x_i^{(k-1)}) + b_j $$\n",
    "  where $d$ is the number of neurons in the layer $k-1$ and $W$ is the weight matrix between the two layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfkl = tf.keras.layers\n",
    "\n",
    "classification_model = tf.keras.Sequential()\n",
    "classification_model.add(\n",
    "    tfkl.Dense(1000, input_shape=(2,),\n",
    "                name=\"hidden_layer\")\n",
    ")\n",
    "classification_model.add(\n",
    "    tfkl.Activation(tf.nn.relu, \n",
    "                    name=\"hidden_activation\")\n",
    ")\n",
    "classification_model.add(\n",
    "    tfkl.Dense(1, name=\"output_layer\")\n",
    ")\n",
    "classification_model.add(\n",
    "    tfkl.Activation(tf.nn.sigmoid, \n",
    "                    name=\"output_activation\")\n",
    ")                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model defined, we just need to *compile* it. This process will create the weights and biases parameters for all the layers and define the loss function, optimization strategy and (optionally) evaluation metrics\n",
    "\n",
    "#### Binary Cross-entropy Loss\n",
    "For binary classification tasks, the prediction error is way better represented by the **binary cross-entropy** than anything else.\n",
    "\n",
    "$$ \\mathcal{L}_\\text{BCE}(y_\\text{true}, y_\\text{pred}) = -[(1-y_\\text{true})\\log(1-y_\\text{pred}) + y_\\text{true}\\log(y_\\text{pred})] $$\n",
    "\n",
    "We basically treat our prediction of a probability of the data to belong to class `1`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.01), \n",
    "                             loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                             metrics=tf.metrics.BinaryAccuracy(threshold=0.5)\n",
    "                             )\n",
    "classification_model.summary()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the generated **summary**, we see all the layers that we defined previously. We also see the number of parameters associated with each one. For example, our *hidden_layer* that takes the 2D data points as input has 1000 output neurons resulting in $2 \\times 1000$ weights and $1000$ biases. The *output_layer* returns our scalar prediction using the previous 1000 neurons, thus we have $1\\times1000 + 1$ parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the neural network\n",
    "\n",
    "As we have previously seen with `sklearn` models, the training loop is integrated in a method called `fit` which takes the training data as positional arguments. On top of that we may also specify the `batch_size`, which represents how many data points to use for a single optimization step. Usually we need way more optimization steps than we get by splitting our dataset in batches. So, in practice we will re-use the same data multiple times, until our model has converged. Those are called `epochs`. For each *epoch* we split the training dataset in batches and do `X_train.shape[0]/batch_size` optimization steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 200\n",
    "\n",
    "# initialize tqdm callback for a progress bar\n",
    "tqdm_callback = tfa.callbacks.TQDMProgressBar(show_epoch_progress=False)\n",
    "\n",
    "history = classification_model.fit(X_train, y_train, \n",
    "                                    epochs=epochs, batch_size=batch_size,\n",
    "                                    verbose=0,\n",
    "                                    callbacks=[tqdm_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation\n",
    "\n",
    "We can take a look of how the **accuracy** and **loss** changed from one epoch to the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.arange(epochs), history.history[\"binary_accuracy\"],\n",
    "        label=\"Binary Crossentropy\")\n",
    "plt.plot(np.arange(epochs), history.history[\"loss\"],\n",
    "        label=\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch number\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `evaluate` method will compute the average loss on the given data alongside with any metrics that were defined in model compilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also take a look at our predictions on the test set by using the `predict` method. Are our predictions aligned with the true values? Are there regions in our input space where the model performs better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classification_model.predict(X_test)\n",
    "fig = plt.figure()\n",
    "plt.contour(u, v, y_binary)\n",
    "plt.scatter(X_test[:,0], X_test[:,1], c=y_pred, s=2)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is deeper better than wider?\n",
    "Write your own classification neural network with a number of total trainable parameters under $1000$ and an aim for an accuracy above $93\\%$. You may change the previous model however you want. Add more layers, change the number of nodes, the loss function, batch size, optimizer, learning rate, it doesn't matter as long as long as you respect the size and accuracy conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError\n",
    "\n",
    "deep_model = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "Using what you learned so far, develop a machine learning model for regression on data generated by `some_2d_function`. This time we are trying to predict the actual function value instead of labeling the values with $1$ or $0$.\n",
    "\n",
    "The code below will generate the data samples and split them into training and testing. Feel free to increase the sample size if you feel you may need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 5_000\n",
    "u_sample3d = np.random.uniform(u_low, u_high, sample_size)\n",
    "v_sample3d = np.random.uniform(v_low, v_high, sample_size)\n",
    "y_sample3d = some_2d_function(u_sample3d, v_sample3d, noise=0.2)\n",
    "X = np.vstack([u_sample3d, v_sample3d]).T\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_sample3d, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3D plots of the true function values and our scattered nosy data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.contour3D(u,v,y,100)\n",
    "ax1.view_init(40, -60)\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "ax2.scatter(u_sample3d, v_sample3d, y_sample3d, s=2, c=y_sample3d)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suggested steps:\n",
    "- **Define the problem:** we know we have a regression to do, but how should we implement it? What would be an appropriate loss function? By which metrics should we evaluate our predictions?\n",
    "- **Define and compile your own model:** How big are the layers? How deep should the network be? What will be our activation functions? What optimizer should we use for gradient descent? Learning rate value?\n",
    "- **Train the model:** Chose the batch size and number of epochs and fit the model.\n",
    "- **Evaluation plots:** Show how the model converged to minimum loss. Show relevant plots for your prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 2\n",
    "\n",
    "Now, let's take a look at a more complex example. We are going to use the **MNIST** dataset corresponding to hand written digits, represented as $28\\times28$ matrices. The matrix values are brightness numbers in $[0, 255]$, you may want to rescale this into the $[0, 1]$ interval\n",
    "\n",
    "<img src=\"resources/mnist.png\" width=600>\n",
    "\n",
    "Create a neural network that predicts the digit represented in the matrix. This is a multi-label classification problem.\n",
    " \n",
    "\n",
    "Tips:\n",
    "- Instead of trying to output a single value from `0` to `9`, which may be hard to interpret, try to have 10 output layers where the values are the probability of the input to correspond to each of 10 classes.\n",
    "- You may do a one-hot encoding of your output to transform your training and test data (e.g. $2 \\rightarrow [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]$) and `CategoricalCrossentropy` would be your loss function.\n",
    "- Alternatively, `SparseCategoricalCrossentropy` loss allows you to use the data output data as it is.\n",
    "- For the output activation, `tf.nn.softmax` may be a good choice ([wiki link](https://en.wikipedia.org/wiki/Softmax_function)).\n",
    "- You can't apply a `Dense` layer to a 2D Tensor. There is a special layer called `Flatten`, which reshapes a matrix into a single vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
